{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torchvision.io import read_image\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights, list_models\nfrom torchvision.datasets import ImageNet, ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.classification import MulticlassAccuracy\nimport torch\nimport time\nimport csv\nimport os\nfrom torchvision.transforms import transforms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from torchvision.io import read_image\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights, list_models\nfrom torchvision.datasets import ImageNet, ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.classification import MulticlassAccuracy\nimport torch\nimport time\nimport csv\nimport os\nfrom torchvision.transforms import transforms","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define all needed abilities to run","metadata":{}},{"cell_type":"code","source":"# Move model and data to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device used is: \" + str(device))\n\n# create a metric for accuracy\nmetric = MulticlassAccuracy(num_classes=1000).to(device)\n\n# Step 2: Initialize the inference transforms\nweights =ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\npreprocess = weights.transforms(antialias=True)\n\npreprocess_w_gray2rgb = transforms.Compose([\n    lambda x: x.expand(3, -1, -1) if x.shape[0] == 1 else x,\n    preprocess\n])\nimagenet_val_dir = '/kaggle/input/imagenet-val/imagenet_val'\ndataset = ImageFolder(root=imagenet_val_dir, loader=read_image, transform=preprocess_w_gray2rgb)\nclass_dict = dataset.class_to_idx\nclass_dict = {value: key for key, value in class_dict.items()}\n\n# Create a DataLoader to load the images in batches\ndataloader = DataLoader(dataset, batch_size=8, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# inference for one image only - verify ","metadata":{}},{"cell_type":"code","source":"img = read_image(\"/kaggle/input/imagenet-val/imagenet_val/10/ILSVRC2012_val_00010763.JPEG\")\nprint(img.shape[0])\nif img.shape[0] == 1:\n     img = img.expand(3, -1, -1)\nprint(img.size())\n\n# Step 1: Initialize model with the best available weights\nweights =ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\nmodel = vit_b_16(weights=weights)\nmodel.eval()\n\n# Step 2: Initialize the inference transforms\npreprocess = weights.transforms()\n\n# Step 3: Apply inference preprocessing transforms\nbatch = preprocess(img).unsqueeze(0)\n\n# Step 4: Use the model and print the predicted category\nprediction = model(batch).squeeze(0).softmax(0)\nclass_id = prediction.argmax().item()\nprint(class_id)\nscore = prediction[class_id].item()\ncategory_name = weights.meta[\"categories\"][class_id]\nprint(f\"{category_name}: {100 * score:.1f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"code","source":"def check_label_name(predictions, weights):\n    for prediction in predictions:\n        class_id = prediction.argmax().item()\n        print(class_id)\n        score = prediction[class_id].item()\n        category_name = weights.meta[\"categories\"][class_id]\n        print(f\"{category_name}: {100 * score:.1f}%\")\n        print(\"\\n\")\n        \n        \ndef collect_data(model_name, accuracy, duration, device):\n    # Calculate model size\n    model_size = os.path.getsize(model_name) / (1024**2)\n\n    # Append data to CSV file\n    with open('model_data.csv', 'a', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([model_name, accuracy, duration, model_size, device])\n        \ndef model_quantization(model, backend='x86', save=False, qtype='int'):\n    model.eval()\n    if qtype == 'int':\n        type_to_quantize = torch.qint8\n    elif qtype == 'uint':\n        type_to_quantize = torch.quint8\n    else:\n        print(\"invalid type, stopping\")\n        exit(1)\n    # Use 'x86' for server inference (the old 'fbgemm' is still available but 'x86' is the recommended default) and ``qnnpack`` for mobile inference.\n    #model.qconfig = torch.quantization.get_default_qconfig(backend)\n    #torch.backends.quantized.engine = backend\n    quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n    #scripted_quantized_model = torch.jit.script(quantized_model)\n    \n    if save:\n        scripted_quantized_model.save(\"vit_scripted_quantized.pt\")\n        \n    return quantized_model\n        \n        \ndef labels_process(labels, class_dict):\n    # Change labels because of dataset idx\n    labels = [class_dict[int(label)] for label in labels]\n    labels = [int(num) for num in labels]\n    labels = torch.tensor(labels)\n    return labels\n    \ndef inference(model, dataloader, class_dict, device, image_num_stop=40000):\n    index_stop = image_num_stop // 8\n    total_correct = 0\n    total_samples = 0\n    start_time = time.time()\n    model.to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        for index, (images, labels) in enumerate(dataloader):\n            images = images.to(device)\n        \n            # Change labels because of dataset idx\n            labels = labels_process(labels, class_dict)\n            labels = labels.to(device)\n        \n            predictions = model(images)\n            predicted_labels = torch.argmax(predictions, dim=1) + 1  # Add 1 to predicted labels\n            \n            total_correct += (predicted_labels == labels).sum().item()\n            total_samples += labels.size(0)\n        \n            if index % 50 == 0:\n                print(\"{} images were processed out of 50,000\".format(8 * index))\n            \n            if index == index_stop:\n                print(\"Number of images processed: {} stopping now\".format(index_stop*8))\n                print(\"stopped checking because of errors for the entire dataset \\n \")\n                break\n            \n    accuracy = total_correct / total_samples\n    end_time = time.time()\n    duration = (end_time - start_time) / 60\n    \n    return accuracy, duration\n\n    \n    \ndef inference_list(model, dataloader, class_dict, device, label_dict, image_num_stop=40000):\n    index_stop = image_num_stop // 8\n    label_counts = {label: {\"correct\": 0, \"total\": 0} for label in range(1,1001)}\n    start_time = time.time()\n    model.to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        for index, (images, labels) in enumerate(dataloader):\n            images = images.to(device)\n        \n            # Change labels because of dataset idx\n            labels = labels_process(labels, class_dict)\n            labels = labels.to(device)\n        \n            predictions = model(images)\n            predicted_labels = torch.argmax(predictions, dim=1) + 1  # Add 1 to predicted labels\n            \n            for pred_label, true_label in zip(predicted_labels, labels):\n                label_counts[true_label.item()][\"total\"] += 1\n                if pred_label == true_label:\n                    label_counts[true_label.item()][\"correct\"] += 1\n            \n            if index % 50 == 0:\n                print(\"{} images were processed out of 50,000\".format(8 * index))\n            \n            if index == index_stop:\n                print(\"Number of images processed: {} stopping now\".format(index_stop * 8))\n                print(\"stopped checking because of errors for the entire dataset \\n \")\n                break\n    \n    accuracies = {}\n    for index, (label, counts) in enumerate(label_counts.items()):\n        accuracies[label_dict[index]] = counts[\"correct\"] / counts[\"total\"] if counts[\"total\"] > 0 else 0.0\n    \n    end_time = time.time()\n    duration = (end_time - start_time) / 60\n    \n    return accuracies, duration\n\n\ndef write_dict_to_csv(dictionary, filename):\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Label', 'Accuracy'])\n        for label, accuracy in dictionary.items():\n            writer.writerow([label, accuracy])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline ViT - ran on GPU","metadata":{}},{"cell_type":"code","source":"# Step 1: Initialize model with the best available weights\nweights = ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\nmodel = vit_b_16(weights=weights)\n\naccuracy, duration = inference(model=model, dataloader=dataloader, class_dict=class_dict, device=device, image_num_stop=10000)\n\nprint(\"Inference took {} minutes\".format(duration))\nprint(\"Accuracy for this model is {}\".format(accuracy))\n\nmodel_name = \"vit_baseline.pth\"\ntorch.save(model, model_name)\ncollect_data(model_name=model_name, accuracy=accuracy, duration=duration, device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dynamic quantization - float16","metadata":{}},{"cell_type":"code","source":"# deosn't work\n# Step 1: Initialize model with the best available weights\nweights = ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\nmodel = vit_b_16(weights=weights)\nquantized_vit = model_quantization(model=model, qtype='float')\n\naccuracy, duration = inference(model=quantized_vit, dataloader=dataloader, class_dict=class_dict, device=\"cpu\", image_num_stop=10000)\n\nprint(\"Inference took {} minutes\".format(duration))\nprint(\"Accuracy for this quantized model is {}\".format(accuracy))\n\nmodel_name = \"vit_basic_quantization.pth\"\ntorch.save(quantized_vit, model_name)\ncollect_data(model_name=model_name, accuracy=accuracy, duration=duration, device=device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dynamic quantization - int8","metadata":{}},{"cell_type":"code","source":"# Step 1: Initialize model with the best available weights\nweights = ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\nmodel = vit_b_16(weights=weights)\nquantized_vit = model_quantization(model=model, qtype='uint')\n\n# make label_dict - turn into a function later\nlabel_list = weights.meta[\"categories\"]\n\naccuracy_list, duration = inference_list(model=model, dataloader=dataloader, class_dict=class_dict, device=\"cpu\", label_dict=label_list, image_num_stop=10000)\n\nprint(\"Inference took {} minutes\".format(duration))\nprint(accuracy_list)\n\nwrite_dict_to_csv(accuracy_list, 'Vit_quantized_uint.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}